\chapter{Advanced Topics}
\label{sec:advanced topics}
\epigraph{The miracle of the appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift, which we neither understand nor deserve. }{\textit{Eugene Wigner}}

This section is non examinable and is included so that you have a feel for how the material in this module can be taken further.

\section{L'H\^{o}pital's rule for evaluating limits}
\label{sec:l'hopital}
We saw in \cref{sec:functions} that some limits lead to nonsense expressions. For example, If we were confronted with $\lim_{x\to 0}\sin(x)/x$, this looks like it will result in $0/0$ which is does not make sense. Remember that not all limits that look indeterminate really are. For example
\begin{equation*}
\lim_{x\to 4}\frac{x^{2}-16}{x-4}
\end{equation*}
looks at first glance like it will have the form $0/0$ since both the numerator and denominator vanish for $x=4$. However, the numerator can be factored as $x^{2}-16=(x-4)(x+4)$ so the limit simplifies to
\begin{equation*}
\lim_{x\to 4}\frac{x^{2}-16}{x-4}=\lim_{x\to 4}\frac{(x-4)(x+4)}{x-4}=\lim_{x\to4}(x+4)=8.
\end{equation*}
This is why we said that you should try to expand and simplify the function that you are taking the limit of as much as possible. \\

L'H\^{o}pital\footnote{Originally spelt L'Hospital, with a silent s and no circumflex}'s rule enable us to make sense of some of those limits which still look indeterminate after they have been simplified. To apply L'H\^{o}pital's rule, we have to be taking the limit of a ratio of functions $f(x),g(x)$, where both are differentiable, the derivative of $g(x)$ does not vanish, and in the limit $f(x)$ and $g(x)$ wither both go to zero or both go to infinity. Then we have that
\begin{equation}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)},
\label{eq: l'hopital's rule}
\end{equation}
so we can replace the ratio of the functions with the ratio of their derivatives. If both of the derivatives still vanish, or diverge, in the limit, then process can be repeated and we end up with more derivatives
\begin{equation*}
\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f^{(n)}(x)}{g^{(n)}(x)},
\end{equation*}
where the superscript $(n)$ means that we are differentiating the functions $n$ times.\\

Note that if the limit does not lead to an indeterminate then L'H\^{o}pital's rule does not hold!

\begin{ex}
As a counter example consider the limit
\begin{equation*}
\lim_{x\to 1}\frac{f(x)}{g(x)}=\lim_{x\to 1}\frac{x+1}{2x+1}.
\end{equation*}
Both $\lim_{x\to 1}f(x)=2$ and $\lim_{x\to1}g(x)=3$ are finite and no-zero. Evaluating the limit directly gives
\begin{equation*}
\lim_{x\to 1}\frac{x+1}{2x+1}=\frac{2}{3}.
\end{equation*}
We can also calculate the limit of the ratio of derivatives,
\begin{equation*}
\lim_{x\to 1}\frac{f'(x)}{g'(x)}\lim_{x\to 1}\frac{1}{2}=\frac{1}{2}\neq \frac{2}{3}.
\end{equation*}
So the limit of the ratio does not match the limit of the ratio of derivatives.
\end{ex}


As long as our original limit looks like it is indeterminate we can use L'H\^{o}pital's rule.

\begin{ex}
The limit of $\sin(x)/x$ as $x\to 0$ is evaluated as follows
\begin{equation*}
\lim_{x\to0}\frac{\sin(x)}{x}=\lim_{x\to0}\frac{\cos(x)}{1}=1.
\end{equation*}
\end{ex}

We can also evaluate other limits that may not at first look like a ratio of functions.
\begin{ex}
Consider the limit $\lim_{x\to-\infty}xe^{x}$ this looks like it becomes the indeterminate $(-\infty)(0)$. If we recall that $1/e^{x}=e^{-x}$ then we can rewrite the limit as
\begin{equation*}
\lim_{x\to-\infty}xe^{x}=\lim_{x\to-\infty}\frac{x}{e^{-x}},
\end{equation*}
which looks like the indeterminate $-\infty/\infty$. Now we can apply L'H\^{o}pital's rule to get
\begin{equation*}
\lim_{x\to-\infty}\frac{x}{e^{-x}}=\lim_{x\to-\infty}\frac{1}{-e^{-x}}=0,
\end{equation*}
since $1/\infty$ is zero.
\end{ex}

\section{Trigonometric Derivatives in Degrees}
At various points in this module I have said that you should always work with angles in radians as this makes life easier when differentiating trig functions. Here I want to discuss how to take the derivative if we are measuring the angle in degrees.\\

First we need some notation, we will use the subscript $\text{rad}$ when our function has an argument in degrees and $\text{deg}$ when our function has an argument in degrees. Then we can assume the result that we were given in \cref{sec:differentiation}, but rewritten in this new notation, that
\begin{equation*}
\frac{\ud }{\ud x}\sin_{\text{rad}}(x)=\cos_{\text{rad}}(x).
\end{equation*}

Now recall that if $x$ is an angle in radians then the corresponding angle, $\theta$ in degrees is
\begin{equation*}
\theta =\frac{180^{\circ}}{\uppi}x, \qquad \text{or } x=\frac{\uppi}{180^{\circ}}\theta.
\end{equation*}

Now we can calculate the derivative of $\sin_{\text{deg}}$ using the chain rule
\begin{align*}
\frac{\ud}{\ud \theta}\sin_{\text{deg}}(\theta)&=\frac{\ud}{\ud \theta}\sin_{\text{rad}}\left(\frac{\uppi}{180^{\circ}}\theta\right)\\
&=\frac{\ud x}{\ud \theta}\frac{\ud }{\ud x}\sin_{\text{rad}}(x)\\
&=\frac{\uppi}{180^{\circ}}\cos_{\text{rad}}(x)\\
&=\frac{\uppi}{180^{\circ}}\cos_{\text{rad}}\left(\frac{\uppi}{180^{\circ}}\theta\right)\\
&=\frac{\uppi}{180^{\circ}}\cos_{\text{deg}}\left(\theta\right).
\end{align*}

The derivatives of other trig functions can be calculated in similar ways.   This is why we have to be careful to use radians as we do not want to have to keep track of factors of $\uppi/180^{\circ}$ every time we differentiate.\\

There is an interesting discussion of this result and how to think about the difference between trig functions that take arguments in radians and degrees \href{https://math.stackexchange.com/questions/214912/derivative-of-the-sine-function-when-the-argument-is-measured-in-degrees}{here}

\section{Functions of two variables}

\section{Multiple integrals}

\section{Optimisation Problems}
In \cref{sec: apps of diff} we discussed finding the critical points of a function by studying where the derivative vanishes. The example that I like to have in mind comes from physics. Consider a ball thrown straight up in the air, how do we find the maximum height that the ball reaches? We look at where the velocity vanishes, since as the ball is moving upwards towards it is being decelerated by gravity so the velocity decreases, then when it is going down after reaching its maximum height the velocity is getting larger but pointing in the opposite direction. For this to happen there has to be a point where the velocity is instantaneously zero. This is at the maximum height. Calculus comes into this as the velocity of an object is the derivative of its displacement, so looking for where the velocity vanishes is exactly the same as finding the critical points where the derivative of the displacement vanishes.\\

We also discussed that critical points correspond to the minima and maxima ( and points of inflection) of a function. This means that if our function describes the area enclosed by a fence, or the time taken to run an algorithm that finding the critical points will tell us the maximum or minimum area that we can enclose, or the fastest and slowest that the algorithm can run. This is what is meant by \textbf{Optimisation}, we are optimising a function by finding the parameter values which make its output the largest or the smallest it can be. \\

\begin{mdiv}
Note that the function can be constrained, e.g. if it is restricted a particular set of values that is not the full range of the function, then it may not achieve its global maximum or minimum. In that case you have to evaluate the function at the boundary of the region and see if the values it takes are larger or smaller than that taken at any critical points which satisfy the constraint.\\

See section 4.8 of \citep{calcI} for more on constrained optimisation at the level relevant to this module.
\end{mdiv}

In other words, optimisation problems are all about finding the maxima, minima, and points of inflection for functions, and interpreting the physical meaning of these points. A famous historical example is the \textbf{Dido problem}\footnote{named after a queen of Carthage} which asks you to find the maximum area that can be enclosed by a shape with a given fixed perimeter. The solution to the Dido problem is the circle.\\

Another similar problem is asking what curve minimises the distance between two points. In flat space we all know the answer is a straight line, but have you ever though about how this changes if the surfaces is curved? It turns out that the solution to the problem changes. This is why aeroplanes fly along sections of a circle, as these give the shortest distance on a globe.

\section{Polynomial approximation}
In \cref{sec:numerics} we met Newton's divided difference method which enabled us to find a polynomial approximation to a function, making use of a table of values with a uniform step size\footnote{We saw that if the step size is not uniform we can still get a similar approximation.}.  The interpolating polynomial is expressed in terms of the data points, step size, and finite differences as
\begin{equation}
f(x)=f_{0}+\frac{x-x_{0}}{h}\Delta f_{0}+\frac{(x-x_{0})(x-x_{1})}{2!h^{2}}\Delta^{2}f_{0}+\dots +\frac{(x-x_{0})(x-x_{1})\cdots (x-x_{n})}{n!h^{n}}\Delta^{n}f_{0}.
\label{eq: divided difference 2}
\end{equation}
The number of terms $n+1$ is determined by the number of data points we start of with as if we only have one point, then we just get $f_{0}$, two data points gives one first difference $\Delta f_{0}=f_{1}-f_{0}$, three data points gives one second difference, $\Delta^{2}f_{0}=\Delta f_{1}-\Delta f_{0}$. Thus $n+1$ points gives a series with $n+1$ terms as shown in \cref{eq: divided difference 2}.\\

The discussion of numerical differentiation in \cref{sec:numerics} showed us that these finite differences are discrete are discrete versions of the derivative. For example
\begin{equation*}
\frac{\ud f}{\ud x}(a)\simeq \frac{f(a+h)-f(a)}{h}=\frac{\Delta f_{a}}{h}
\end{equation*}
in the forward difference scheme.  It turns out that we can do the same for higher derivatives and see that 
\begin{equation*}
\frac{\ud^{2}f}{\ud x^{2}}(a)\simeq \frac{\Delta^{2}f_{a}}{h^{2}},
\end{equation*}
or in general
\begin{equation*}
\frac{\Delta^{n}f_{a}}{h^{n}}=\frac{\ud^{n}f}{\ud x^{n}}(a)
\end{equation*}

